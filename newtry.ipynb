{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running scGPT annotation...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GeneVocab' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 300\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcell_type_annotation\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRunning scGPT annotation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 300\u001b[0m scgpt_pred \u001b[38;5;241m=\u001b[39m \u001b[43mscgpt_annotation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscgpt_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscgpt_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    301\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscgpt\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m: scgpt_pred,\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: accuracy_score(scgpt_query\u001b[38;5;241m.\u001b[39mobs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCelltype\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcat\u001b[38;5;241m.\u001b[39mcodes, scgpt_pred),\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m: f1_score(scgpt_query\u001b[38;5;241m.\u001b[39mobs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCelltype\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcat\u001b[38;5;241m.\u001b[39mcodes, scgpt_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    305\u001b[0m }\n",
      "Cell \u001b[1;32mIn[13], line 59\u001b[0m, in \u001b[0;36mscgpt_annotation\u001b[1;34m(ref_adata, query_adata)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pad_token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m vocab:\n\u001b[0;32m     58\u001b[0m     vocab\u001b[38;5;241m.\u001b[39madd_token(pad_token)\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvailable vocab keys:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mvocab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m())\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPad Token:\u001b[39m\u001b[38;5;124m\"\u001b[39m, pad_token)\n\u001b[0;32m     61\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GeneVocab' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "import TOSICA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from run import config\n",
    "from scgpt.model import TransformerModel\n",
    "from scgpt.tokenizer import tokenize_and_pad_batch, GeneVocab\n",
    "from scgpt.preprocess import Preprocessor\n",
    "import wandb\n",
    "import json\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "project = config['project']\n",
    "pretrained_path = os.path.normpath(os.path.dirname(config['scgpt_params']['pretrained_path']))\n",
    "scgpt_ref=config['ref_dataset_path']\n",
    "scgpt_query=config['query_dataset_path']\n",
    "def pre_process(data_path, project):\n",
    "    \n",
    "    if model_name == 'TOSICA':\n",
    "        project=project\n",
    "        adata = sc.read(data_path)\n",
    "        adata = adata[:, adata.var_names]\n",
    "        return adata\n",
    "    elif model_name=='scgpt':\n",
    "        project=project\n",
    "        adata = sc.read(data_path)\n",
    "        preprocessor = Preprocessor(\n",
    "            use_key=\"X\",\n",
    "            filter_gene_by_counts=False,\n",
    "            normalize_total=1e4,\n",
    "            log1p=True,\n",
    "            binning=config['scgpt_params']['n_bins']\n",
    "        )\n",
    "        preprocessor(adata)\n",
    "        adata.obs['Celltype'] = adata.obs['Celltype'].astype('category')\n",
    "        \n",
    "    return adata\n",
    "\n",
    "def scgpt_annotation(ref_adata, query_adata):\n",
    "    \"\"\"scGPT cell type annotation pipeline\"\"\"\n",
    "    \n",
    "    # Load gene vocab\n",
    "    vocab_path = os.path.join(pretrained_path, \"vocab.json\")\n",
    "    with open(vocab_path, 'r') as file:\n",
    "        gene_list = json.load(file)\n",
    "    \n",
    "    gene_names = [gene for gene, _ in gene_list.items()]\n",
    "    vocab = GeneVocab(gene_list=gene_names)\n",
    "    \n",
    "    # Add padding token if missing\n",
    "    pad_token = '<pad>'\n",
    "    if pad_token not in vocab:\n",
    "        vocab.add_token(pad_token)\n",
    "    print(\"Available vocab keys:\", vocab.keys())\n",
    "    print(\"Pad Token:\", pad_token)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    query_adata=sc.read(config['query_dataset_path'])\n",
    "    celltype_categories = query_adata.obs['Celltype'].cat.categories\n",
    "\n",
    "    model = TransformerModel(\n",
    "        ntoken=len(vocab),\n",
    "        d_model=128,\n",
    "        nhead=4,\n",
    "        d_hid=128,\n",
    "        nlayers=4,\n",
    "        nlayers_cls=config['scgpt_params']['n_layers_cls'],\n",
    "        n_cls=len(celltype_categories)  # Fix: use correct category length\n",
    "    ).to(device)\n",
    "\n",
    "    # Load pretrained model weights\n",
    "    model.load_state_dict(torch.load(config['scgpt_params']['pretrained_path'] + \"/best_model.pt\"))\n",
    "\n",
    "    def prepare_scgpt_batches(adata):\n",
    "        \"\"\"Tokenize and batch scGPT data\"\"\"\n",
    "        gene_ids = np.array([vocab[gene] for gene in adata.var_names], dtype=int)\n",
    "        tokenized = tokenize_and_pad_batch(\n",
    "            adata.layers['X_binned'],\n",
    "            gene_ids,\n",
    "            max_len=3001,\n",
    "            vocab=vocab,\n",
    "            pad_value=vocab['<pad>']\n",
    "        )\n",
    "        return DataLoader(SeqDataset(tokenized), batch_size=config['scgpt_params']['batch_size'])\n",
    "\n",
    "    # Fine-tuning model\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['scgpt_params']['lr'])\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    train_loader = prepare_scgpt_batches(train_data)\n",
    "    for epoch in range(config['scgpt_params']['epochs']):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            inputs = batch['gene_ids'].to(device)\n",
    "            values = batch['values'].to(device)\n",
    "            labels = batch['Celltype_labels'].to(device)\n",
    "            \n",
    "            outputs = model(inputs, values)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Predict on query data\n",
    "    query_loader = prepare_scgpt_batches(query_data)\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in query_loader:\n",
    "            inputs = batch['gene_ids'].to(device)\n",
    "            values = batch['values'].to(device)\n",
    "            outputs = model(inputs, values)\n",
    "            predictions.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def pre_train(ref_adata, epochs, project):\n",
    "    TOSICA.train(ref_adata, gmt_path='human_gobp', label_name='Celltype', epochs=epochs, project=project)\n",
    "    return f'./{project}/model-0.pth'\n",
    "\n",
    "\n",
    "def fine_tune(model_weight_path, query_adata, project):\n",
    "    model_weight_path= f'./{project}/model-0.pth'\n",
    "    new_adata = TOSICA.pre(query_adata, model_weight_path=model_weight_path, project=project)\n",
    "    new_adata.write('tosica_att.h5ad')\n",
    "    return new_adata\n",
    "\n",
    "\n",
    "def evaluate(predictions, labels):\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    return accuracy, f1\n",
    "\n",
    "def extract_latent_representation(adata):\n",
    "    # Extract latent representations from TOSICA if available\n",
    "    adata = tosica_model.preprocess_data(adata)\n",
    "    latent_representations = tosica_model.get_latent_representation(adata)\n",
    "    adata.obsm['X_latent'] = latent_representations\n",
    "    return adata\n",
    "\n",
    "def save_results(results, output_csv):\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "\n",
    "def perform_umap_and_clustering(new_adata):\n",
    "    embeddings = new_adata.obsm['X_latent']  # Adjust based on how embeddings are stored\n",
    "    reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='euclidean')\n",
    "    umap_embeddings = reducer.fit_transform(embeddings)\n",
    "\n",
    "    # Clustering with K-Means\n",
    "    n_clusters = 5  # Adjust based on your data\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    new_adata.obs['kmeans_labels'] = kmeans.fit_predict(umap_embeddings)\n",
    "\n",
    "    # Visualization\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(umap_embeddings[:, 0], umap_embeddings[:, 1], c=new_adata.obs['kmeans_labels'], cmap='Spectral', s=5)\n",
    "    plt.title('UMAP Projection of Cell Embeddings with K-Means Clustering')\n",
    "    plt.xlabel('UMAP 1')\n",
    "    plt.ylabel('UMAP 2')\n",
    "    plt.colorbar(label='Cluster Label')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#  Introduce perturbations\n",
    "def introduce_noise(adata, noise_level=0.1):\n",
    "    if not isinstance(adata.X, np.ndarray):\n",
    "        adata.X = adata.X.toarray()\n",
    "    adata.X = adata.X.astype(np.float32)\n",
    "    noise = np.random.normal(0, noise_level, adata.X.shape).astype(np.float32)\n",
    "    adata.X += noise\n",
    "    adata.X = adata.X.astype(np.int32)\n",
    "    return adata\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_name = config['model_name']\n",
    "    task = config['task']\n",
    "    ref_dataset_path = config['ref_dataset_path']\n",
    "    query_dataset_path = config['query_dataset_path']\n",
    "    epochs = config['epochs'] \n",
    "    model_weight_path = f'./{project}/model-0.pth'\n",
    "    results = []\n",
    "\n",
    "    if model_name == \"TOSICA\":\n",
    "        if task == 'cell_type_annotation':\n",
    "            # Perform pre-training, fine-tuning, and evaluation\n",
    "            ref_adata = pre_process(ref_dataset_path, project=project)\n",
    "            model_path = pre_train(ref_adata, epochs=epochs, project=project)\n",
    "            query_adata = pre_process(query_dataset_path, project=project)\n",
    "            new_adata = fine_tune(model_weight_path, query_adata, project=project)\n",
    "            labels = new_adata.obs['Celltype'].values\n",
    "            print(labels)\n",
    "            predictions = new_adata.obs['Prediction'].values\n",
    "            print(predictions)\n",
    "            acc, f1 = evaluate(predictions, labels)\n",
    "            results.append({'task': task, 'accuracy': acc, 'f1_score': f1})\n",
    "            new_adata.raw = new_adata\n",
    "            sc.pp.normalize_total(new_adata, target_sum=1e4)\n",
    "            sc.pp.log1p(new_adata)\n",
    "            sc.pp.scale(new_adata, max_value=10)\n",
    "            sc.tl.pca(new_adata, svd_solver='arpack')\n",
    "            sc.pp.neighbors(new_adata, n_neighbors=10, n_pcs=40)\n",
    "            sc.tl.umap(new_adata)\n",
    "            col = np.array([\n",
    "                \"#98DF8A\",\"#E41A1C\" ,\"#377EB8\", \"#4DAF4A\" ,\"#984EA3\" ,\"#FF7F00\" ,\"#FFFF33\" ,\"#A65628\" ,\"#F781BF\" ,\"#999999\",\"#1F77B4\",\"#FF7F0E\",\"#279E68\",\"#FF9896\"\n",
    "                    ]).astype('<U7')\n",
    "            prediction_categories = new_adata.obs['Prediction'].unique()\n",
    "            celltype_categories = new_adata.obs['Celltype'].unique()\n",
    "            new_adata.obs['Prediction'] = pd.Categorical(new_adata.obs['Prediction'], categories=prediction_categories)\n",
    "            new_adata.obs['Celltype'] = pd.Categorical(new_adata.obs['Celltype'], categories=celltype_categories)\n",
    "            common_categories = list(set(prediction_categories) & set(celltype_categories))\n",
    "            new_adata.obs['Prediction'] = new_adata.obs['Prediction'].cat.set_categories(common_categories)\n",
    "            new_adata.obs['Celltype'] = new_adata.obs['Celltype'].cat.set_categories(common_categories)\n",
    "            prediction_order = new_adata.obs['Prediction'].value_counts().index\n",
    "            new_adata.obs['Prediction'] = new_adata.obs['Prediction'].cat.reorder_categories(prediction_order)\n",
    "            # Plot UMAP with 'Celltype' categories\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sc.pl.umap(new_adata, color='Celltype', palette='Set1', title='UMAP colored by Celltype', size=50)\n",
    "            plt.show()\n",
    "            latent_representations = TOSICA.get_latent_representation(ref_adata)\n",
    "            kmeans = KMeans(n_clusters=10)\n",
    "            cluster_labels = kmeans.fit_predict(latent_representations)\n",
    "            ref_adata.obs['cluster'] = cluster_labels\n",
    "\n",
    "            # Step 6: Visualization\n",
    "            umap_embeddings = umap.UMAP().fit_transform(latent_representations)\n",
    "            plt.scatter(umap_embeddings[:, 0], umap_embeddings[:, 1], c=cluster_labels, cmap='Spectral', s=5)\n",
    "            plt.colorbar()\n",
    "            plt.title(\"UMAP Visualization of Latent Representations\")\n",
    "            plt.show()\n",
    "            # Ensure both 'Prediction' and 'Celltype' have the same categories\n",
    "             \n",
    "            ## common_categories = list(set(prediction_categories) & set(celltype_categories))\n",
    "            # new_adata.obs['Prediction'] = new_adata.obs['Prediction'].cat.set_categories(common_categories)\n",
    "            # new_adata.obs['Celltype'] = new_adata.obs['Celltype'].cat.set_categories(common_categories)\n",
    "\n",
    "            # new_adata.obs['Prediction'] = new_adata.obs['Prediction'].astype('category')\n",
    "            # print( 'prediction ='+new_adata.obs['Prediction'].cat.categories)\n",
    "            # print(\"____________________________________________________\")\n",
    "            # print(celltype)\n",
    "            # new_adata.obs['Prediction'] = new_adata.obs['Prediction'].cat.reorder_categories(list(celltype))\n",
    "            # new_adata.uns['Prediction_colors'] = col[1:]\n",
    "            \n",
    "            #  celltype = new_adata.obs['Celltype'].values\n",
    "            #  new_adata.obs['Celltype'] = new_adata.obs['Celltype'].astype('category')\n",
    "            #  new_adata.obs['Celltype'] = new_adata.obs['Celltype'].cat.reorder_categories(list(celltype))\n",
    "            # new_adata.uns['Celltype_colors'] = col[:11]\n",
    "            \n",
    "        elif task == 'perturbation':\n",
    "            query_adata = pre_process(query_dataset_path, project=project)  \n",
    "            query_adata = introduce_noise(query_adata, noise_level=0.1)\n",
    "            new_adata = fine_tune(model_weight_path, query_adata, project=project)\n",
    "\n",
    "        elif task == 'clustering':\n",
    "                query_adata = pre_process(query_dataset_path, project=project)\n",
    "                \n",
    "                try:\n",
    "                    new_adata = sc.read('tosica_att.h5ad')\n",
    "                    print(\"Using previously fine-tuned data.\")\n",
    "                except FileNotFoundError:\n",
    "                    print(\"No previous fine-tuned data found. Performing pre-training and fine-tuning.\")\n",
    "                    ref_adata = pre_process(ref_dataset_path,project=project)\n",
    "                    model_path = pre_train(ref_adata, epochs=epochs, project=project)  \n",
    "                    new_adata = fine_tune(model_weight_path, query_adata, project=project)\n",
    "                    new_adata.write('tosica.h5ad')\n",
    "                    results.append({'task': task, 'perturbation': 'Noise introduced and fine-tuning performed'})\n",
    "                perform_umap_and_clustering(new_adata)\n",
    "                results.append({'task': task, 'clustering': 'UMAP and KMeans performed'})\n",
    "        elif task == 'latent_representation':\n",
    "                # Load and process the query dataset\n",
    "                query_adata = pre_process(query_dataset_path, project=project)\n",
    "                query_adata = extract_latent_representation(query_adata)\n",
    "                perform_umap_and_clustering(query_adata)\n",
    "                results.append({'task': task, 'latent_representation_shape': query_adata.obsm['X_latent'].shape})\n",
    "        elif task == 'gene_and_cell_embeddings':\n",
    "                query_adata = pre_process(query_dataset_path, project=project)\n",
    "                try:\n",
    "                    new_adata = sc.read('tosica_att.h5ad')\n",
    "                    print(\"Using previously fine-tuned data.\")\n",
    "                except FileNotFoundError:\n",
    "                    print(\"No previous fine-tuned data found. Performing pre-training and fine-tuning.\")\n",
    "                    ref_adata = pre_process(ref_dataset_path, project=project)\n",
    "                    model_path = pre_train(ref_adata, epochs=epochs, project=project)\n",
    "                    new_adata = fine_tune(model_weight_path, query_adata, project=project)\n",
    "                    \n",
    "                embeddings = new_adata.obsm['X']  # Adjust based on how embeddings are stored\n",
    "                results.append({'task': task, 'embeddings_shape': embeddings.shape})\n",
    "\n",
    "        save_results(results, 'C:/Users/gaiacronus/Downloads/work/combine/modelresults/results.csv')\n",
    "        \n",
    "    else:\n",
    "        if task == 'cell_type_annotation':\n",
    "            print(\"\\nRunning scGPT annotation...\")\n",
    "        scgpt_pred = scgpt_annotation(scgpt_ref, scgpt_query)\n",
    "        results['scgpt'] = {\n",
    "            'predictions': scgpt_pred,\n",
    "            'accuracy': accuracy_score(scgpt_query.obs['Celltype'].cat.codes, scgpt_pred),\n",
    "            'f1': f1_score(scgpt_query.obs['Celltype'].cat.codes, scgpt_pred, average='weighted')\n",
    "        }\n",
    "            \n",
    "        \n",
    "        \n",
    "      \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
